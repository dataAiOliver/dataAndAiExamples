{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(openai_source):\n",
    "    # get llm\n",
    "    if openai_source == \"azure\":\n",
    "        print(f\"USE AZURE AS SOURCE\")\n",
    "        from langchain_openai import AzureChatOpenAI\n",
    "        # https://github.com/langchain-ai/langchain/issues/3137\n",
    "        llm = AzureChatOpenAI(azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "                        , azure_deployment=os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\"), model=\"gpt-35-turbo\"\n",
    "                        , api_version=os.getenv(\"OPENAI_API_VERSION\"), openai_api_type=\"azure\")\n",
    "    elif openai_source == \"openai\":\n",
    "        print(f\"USE OPENAI AS SOURCE\")\n",
    "        from langchain_openai import OpenAI\n",
    "        llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "    else:\n",
    "        print(\"Error: Provide Proper LLM-Source.\")\n",
    "        return None\n",
    "    \n",
    "    return llm\n",
    "    \n",
    "def get_vector_store(fp_pdfs):\n",
    "\n",
    "    # read pdfs\n",
    "    data_pdfs = PyPDFDirectoryLoader(fp_pdfs).load()\n",
    "\n",
    "    # split in chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(data_pdfs)\n",
    "\n",
    "    # embedding\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # vectors for storing\n",
    "    return FAISS.from_documents(text_chunks, embedding=embeddings)\n",
    "\n",
    "def get_qa(fp_pdfs, openai_source):\n",
    "    llm = get_llm(openai_source)\n",
    "    vector_store = get_vector_store(fp_pdfs)\n",
    "    rag_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))\n",
    "    \n",
    "    return rag_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_pdfs = r\"C:\\Users\\oliver.koehn\\Documents\\gitProjects\\dataAndAiExamples\\ragSimple\\in\\pdfs\"\n",
    "qa = get_qa(fp_pdfs, \"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a query to ask the system\n",
    "query = \"How many pokemon are there?\"\n",
    "# run the system and get a response\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
