{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import nest_asyncio  # noqa: E402\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_parse_data(fp_pdf):\n",
    "    fp__parsed_data = \"./data/parsed_data.pkl\"\n",
    "\n",
    "    if os.path.exists(fp__parsed_data):\n",
    "        print(\"PKL-FILE ALREADY EXISTS.\")\n",
    "        # Load the parsed data from the file\n",
    "        parsed_data = joblib.load(fp__parsed_data)\n",
    "    else:\n",
    "        print(\"PKL-FILE DOESNT EXIST YET. CREATE IT.\")\n",
    "        # Perform the parsing step and store the result in llama_parse_documents\n",
    "        parsingInstructionUber10k = \"\"\"The document offers an extensive overview of the UBER's financial performance for the\n",
    "        fourth quarter of 2022. It includes unaudited financial statements, management discussion and analysis, and other relevant\n",
    "        disclosures as required by the SEC. The form features several tables for detailed examination. Please provide precise\n",
    "        responses when addressing the questions contained within.\"\"\"\n",
    "        parser = LlamaParse(api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "                            result_type=\"markdown\",\n",
    "                            parsing_instruction=parsingInstructionUber10k,\n",
    "                            max_timeout=5000,)\n",
    "        llama_parse_documents = parser.load_data(fp_pdf)\n",
    "\n",
    "        # Save the parsed data to a file\n",
    "        print(\"SAVE PARSING RESULT IN PICKEL FILE.\")\n",
    "        joblib.dump(llama_parse_documents, fp__parsed_data)\n",
    "\n",
    "        # Set the parsed data to the variable\n",
    "        parsed_data = llama_parse_documents\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "def remove_non_utf8_characters(source_file_path, cleaned_file_path):\n",
    "    with open(source_file_path, 'rb') as file:\n",
    "        bytes_data = file.read()\n",
    "\n",
    "    # Attempt to decode with 'utf-8' and replace errors with a space or nothing\n",
    "    clean_text = bytes_data.decode('utf-8', errors='replace')\n",
    "\n",
    "    # Write the cleaned text back to a new file\n",
    "    with open(cleaned_file_path, 'w', encoding='utf-8') as clean_file:\n",
    "        clean_file.write(clean_text)\n",
    "\n",
    "    print(f\"Cleaned file saved as {cleaned_file_path}\")\n",
    "\n",
    "# Create vector database\n",
    "def create_vector_database(fp_pdf):\n",
    "    \"\"\"\n",
    "    This function creates a vector database using document loaders and embeddings.\n",
    "    \"\"\"\n",
    "    # Call the function to either load or parse the data\n",
    "    llama_parse_documents = load_or_parse_data(fp_pdf)\n",
    "\n",
    "    markdown_path = \"data/output.md\"\n",
    "    with open(markdown_path, 'w') as f:\n",
    "        for doc in llama_parse_documents:\n",
    "            f.write(doc.text + '\\n')\n",
    "\n",
    "    remove_non_utf8_characters(markdown_path, markdown_path)\n",
    "    \n",
    "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "    documents = loader.load()\n",
    "    # Split loaded documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Initialize Embeddings\n",
    "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    # Create and persist a Chroma vector database from the chunked documents\n",
    "    vs = Chroma.from_documents(documents=docs, embedding=embed_model\n",
    "                               , persist_directory=\"chroma_db_llamaparse1\", collection_name=\"rag\")\n",
    "\n",
    "    return vs, embed_model\n",
    "\n",
    "fp_pdf = rf\"C:\\Users\\{os.getlogin()}\\Documents\\gitProjects\\dataAndAiExamples\\ragComplexPdfs\\in\\Uber-Q4-22-Earnings-Press-Release.pdf\"\n",
    "vectorstore, embed_model = create_vector_database(fp_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you wanna use GROQ\n",
    "chat_model = ChatGroq(temperature=0\n",
    "                      , model_name=\"mixtral-8x7b-32768\"\n",
    "                      , api_key=os.getenv(\"GROQ_API_KEY\"),)\n",
    "\n",
    "retriever=vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt(custom_prompt_template):\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "prompt = set_custom_prompt(custom_prompt_template)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=chat_model,\n",
    "                               chain_type=\"stuff\",\n",
    "                               retriever=retriever,\n",
    "                               return_source_documents=True,\n",
    "                               chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa.invoke({\"query\": \"What is the segment adjusted ebitda of mobility end of 2021 and 2022?\"})\n",
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa.invoke({\"query\": \"What are the Non GAAP Research and development expenses?\"})\n",
    "print(response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
