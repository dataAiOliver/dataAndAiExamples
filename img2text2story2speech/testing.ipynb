{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os, requests\n",
    "from IPython.display import Audio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2text(fp, mock = False):\n",
    "    if mock: return \"A peace of bacon.\"\n",
    "    pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "    text = pipe(fp)[0][\"generated_text\"]\n",
    "    return text\n",
    "\n",
    "mock = False\n",
    "prompt = img2text(os.path.join(\"in\", \"image.jpg\"), mock)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tellAStory(openai_source, prompt, mock = False):\n",
    "    if mock: return \"This is a 20 words story about bacon.\"\n",
    "    if openai_source == \"azure\":\n",
    "        print(f\"USE AZURE AS SOURCE\")\n",
    "        from langchain_openai import AzureChatOpenAI\n",
    "        # https://github.com/langchain-ai/langchain/issues/3137\n",
    "        llm = AzureChatOpenAI(azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "                        , azure_deployment=os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\"), model=\"gpt-35-turbo\"\n",
    "                        , api_version=os.getenv(\"OPENAI_API_VERSION\"), openai_api_type=\"azure\")\n",
    "    elif openai_source == \"openai\":\n",
    "        print(f\"USE OPENAI AS SOURCE\")\n",
    "        from langchain_openai import OpenAI\n",
    "        llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "    else:\n",
    "        print(\"Error: Provide Proper LLM-Source.\")\n",
    "        return None\n",
    "\n",
    "    template = \"\"\"Please tell me a 30 words story about the provided context. CONTEXT: {scenario} STORY:\"\"\"\n",
    "\n",
    "    llm_prompt = PromptTemplate(input_variables=[\"scenario\"],template=template,)\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=llm_prompt)\n",
    "\n",
    "    return chain.run(prompt)\n",
    "\n",
    "story = tellAStory(\"azure\", prompt, mock)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload, API_URL = \"https://api-inference.huggingface.co/models/espnet/kan-bayashi_ljspeech_vits\", mock = False):\n",
    "    if mock:\n",
    "        with open(os.path.join(\"in\", \"audio.flac\"), \"rb\") as f: audio = f.read()\n",
    "        return audio\n",
    "    headers = {\"Authorization\": f'Bearer {os.getenv(\"HUGGINGFACE_API_TOKEN\")}'}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.content\n",
    "\n",
    "audio = query({\"inputs\": story}, mock = mock)\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save current audio for mock\n",
    "#with open(os.path.join(\"in\", \"audio.flac\"), \"wb\") as f: f.write(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play audio\n",
    "Audio(audio, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
